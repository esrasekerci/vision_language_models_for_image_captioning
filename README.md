# Vision-Language Models for Image Captioning

This repository contains the codebase, documentation, and experiment tracking for the final project of **DI 725: Transformers and Attention-Based Deep Networks** at Middle East Technical University.

## Project Overview

This project aims to explore and enhance the **PaliGemma** vision-language foundation model for the task of **image captioning** on remote sensing data. The primary objective is to fine-tune the model on the **RISC dataset**, implement performance-improving strategies, and perform benchmarking against baseline results.

We follow a structured three-phase research plan:
1.  Literature review & project proposal
2.  Preliminary results & benchmarking
3.  Final implementation & comprehensive evaluation

## Repository Structure

```bash
├── data/                  # Folder containing dataset
├── notebooks/             # Final Jupyter notebook for experiments
├── source/                # Source code and custom modules
├── reports/               # IEEE-format project reports
├── figures/               # Figures used in reporting
├── requirements.txt       # Environment dependencies
└── README.md              # Project overview
